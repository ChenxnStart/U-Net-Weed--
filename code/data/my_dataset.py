import os
import torch
import numpy as np
import cv2
from torch.utils.data import Dataset, DataLoader

class MyDatasetInterface:
    def __init__(self, dataset_params, name=None):
        # --- 1. å‚æ•°è§£æž (å…¼å®¹åˆ—è¡¨å’Œå•å€¼) ---
        self.root = dataset_params['root']
        if isinstance(self.root, list): self.root = self.root[0]

        self.batch_size = dataset_params['batch_size']
        if isinstance(self.batch_size, list): self.batch_size = self.batch_size[0]

        self.num_workers = dataset_params.get('num_workers', 4)
        if isinstance(self.num_workers, list): self.num_workers = self.num_workers[0]

        # --- 2. é€šé“é…ç½® ---
        self.channels_config = dataset_params.get('channels', ['rgb'])
        # å¤„ç†åµŒå¥—åˆ—è¡¨ [['rgb', ...]]
        if len(self.channels_config) > 0 and isinstance(self.channels_config[0], list):
            self.channels_config = self.channels_config[0]
        
        # ç¡®ä¿ rgb å­˜åœ¨
        lower_channels = [c.lower() for c in self.channels_config]
        if 'rgb' not in lower_channels:
            print("è­¦å‘Š: è‡ªåŠ¨æ·»åŠ  'rgb' é€šé“")
            self.channels_config.insert(0, 'rgb')
            
        # --- 3. å›¾ç‰‡å°ºå¯¸ ---
        self.img_size = dataset_params.get('img_size', 512)
        if isinstance(self.img_size, list): self.img_size = self.img_size[0]
        
        print(f"â„¹ï¸ æ•°æ®é›†é…ç½®å®Œæˆ: å°ºå¯¸ {self.img_size}x{self.img_size}, æ‰¹é‡ {self.batch_size}")

        # --- 4. åˆå§‹åŒ–æ•°æ®é›† ---
        self.trainset = ChannelSeparatedDataset(self.root, split='train', channels=self.channels_config, img_size=self.img_size)
        
        # éªŒè¯é›†
        if os.path.exists(os.path.join(self.root, 'val')):
            self.valset = ChannelSeparatedDataset(self.root, split='val', channels=self.channels_config, img_size=self.img_size)
        else:
            self.valset = self.trainset

    def build_data_loaders(self, **kwargs):
        self.train_loader = DataLoader(self.trainset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=True)
        self.val_loader = DataLoader(self.valset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=True)

class ChannelSeparatedDataset(Dataset):
    def __init__(self, root, split='train', channels=['rgb'], img_size=512):
        self.split_dir = os.path.join(root, split)
        self.masks_dir = os.path.join(self.split_dir, 'masks')
        self.channels = channels
        self.img_size = img_size
        
        if os.path.exists(self.masks_dir):
            all_masks = sorted([f for f in os.listdir(self.masks_dir) if f.endswith(('.png', '.jpg', '.tif', '.bmp'))])
        else:
            all_masks = []
            print(f"é”™è¯¯: Masks ç›®å½•ä¸å­˜åœ¨: {self.masks_dir}")
            
        self.filenames = []
        self.valid_paths_cache = {} 
        
        # --- å»ºç«‹ç´¢å¼• (è‡ªåŠ¨åŒ¹é…åŽç¼€) ---
        print(f"æ­£åœ¨ç´¢å¼• {split} é›†...")
        for mask_name in all_masks:
            is_valid = True
            file_stem = os.path.splitext(mask_name)[0]
            current_paths = {}

            for channel_name in self.channels:
                folder_name = channel_name.lower()
                target_folder = os.path.join(self.split_dir, folder_name)
                
                found_path = None
                # å°è¯•å¤šç§åŽç¼€
                for ext in ['.tif', '.png', '.jpg', '.TIF', '.PNG', '.JPG']:
                    potential_path = os.path.join(target_folder, file_stem + ext)
                    if os.path.exists(potential_path):
                        found_path = potential_path
                        break
                
                if found_path is None:
                    is_valid = False
                    break
                else:
                    current_paths[channel_name] = found_path
            
            if is_valid:
                self.filenames.append(mask_name)
                self.valid_paths_cache[mask_name] = current_paths
        
        print(f"âœ… {split} é›†æœ‰æ•ˆæ ·æœ¬: {len(self.filenames)}")

    def __len__(self):
        return len(self.filenames)

    def __getitem__(self, idx):
        mask_name = self.filenames[idx]
        paths_map = self.valid_paths_cache[mask_name]
        
        data_list = []
        
        # --- 1. è¯»å–å¤šé€šé“å›¾ç‰‡ ---
        for channel_name in self.channels:
            img_path = paths_map[channel_name]
            img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)
            
            if img is None:
                # æžå°‘æ•°æƒ…å†µæ–‡ä»¶å¯èƒ½æŸå
                raise ValueError(f"æ— æ³•è¯»å–å›¾ç‰‡: {img_path}")
            
            # RGB è½¬ç 
            if channel_name.lower() == 'rgb':
                if img.ndim == 3: img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                else: img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
            
            # è¡¥é½ç»´åº¦ (H, W) -> (H, W, 1)
            if img.ndim == 2:
                img = img[:, :, np.newaxis]
            
            data_list.append(img)

        # åˆå¹¶
        combined_img = np.concatenate(data_list, axis=2)
        
        # --- 2. Resize å›¾ç‰‡ (çº¿æ€§æ’å€¼) ---
        orig_h, orig_w = combined_img.shape[:2]
        if orig_h != self.img_size or orig_w != self.img_size:
            combined_img = cv2.resize(combined_img, (self.img_size, self.img_size), interpolation=cv2.INTER_LINEAR)
            # Resize åŽå•é€šé“ä¼šå˜å›ž (H, W)ï¼Œéœ€è¦å†è¡¥é½
            if combined_img.ndim == 2:
                combined_img = combined_img[:, :, np.newaxis]

        # å½’ä¸€åŒ–
        combined_img = combined_img.astype('float32')
        if combined_img.max() > 255:
            combined_img /= 65535.0
        else:
            combined_img /= 255.0
            
        image_tensor = torch.from_numpy(combined_img).permute(2, 0, 1)

        # --- 3. è¯»å– Mask ---
        mask_path = os.path.join(self.masks_dir, mask_name)
        mask = cv2.imread(mask_path, 0) # è¯»å–ä¸ºç°åº¦
        
        # --- 4. Resize Mask (æœ€è¿‘é‚»æ’å€¼) ---
        if mask.shape[0] != self.img_size or mask.shape[1] != self.img_size:
            mask = cv2.resize(mask, (self.img_size, self.img_size), interpolation=cv2.INTER_NEAREST)
            
        # --- ðŸ”´ 5. å…³é”®ä¿®å¤ï¼šæ¸…æ´—æ ‡ç­¾å€¼ ---
        # æ‚¨çš„ Mask å¯èƒ½æ˜¯ 0(é»‘) å’Œ 255(ç™½)
        # è¿™é‡Œçš„ä»£ç æŠŠæ‰€æœ‰å¤§äºŽ 0 çš„å€¼éƒ½å˜æˆ 1
        mask[mask > 0] = 1
        
        # å†æ¬¡ä¿é™©ï¼šå¦‚æžœè¿˜æœ‰å¤§äºŽç­‰äºŽ 2 çš„å€¼ï¼Œå¼ºåˆ¶å˜æˆ 0
        mask[mask >= 2] = 0
        # -------------------------------

        mask_tensor = torch.from_numpy(mask).long()

        return image_tensor, mask_tensor
